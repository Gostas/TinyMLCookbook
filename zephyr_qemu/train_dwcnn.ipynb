{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization numpy tensorflow"
      ],
      "metadata": {
        "id": "7JbfX1Zvawvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89162972-046b-4da4-bba9-c621cec63ca6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-model-optimization) (0.1.9)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.12/dist-packages (from tensorflow-model-optimization) (1.17.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train base model on CIFAR-10"
      ],
      "metadata": {
        "id": "Y1DgRUSelsgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RQF-lRsKn68H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c773ae-6e72-42d6-bbb7-be329a8c1a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(train_imgs, train_lbls), (val_imgs, val_lbls) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_imgs, val_imgs = train_imgs/255.0, val_imgs/255.0\n",
        "\n",
        "test_imgs, test_lbls = val_imgs[-2000:], val_lbls[-2000:]\n",
        "val_imgs, val_lbls = val_imgs[:8000], val_lbls[:8000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "\n",
        "def separable_conv(i, ch):\n",
        "  x = keras.layers.DepthwiseConv2D((3,3), padding='same')(i)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Conv2D(ch, (1,1), padding='same')(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  return keras.layers.Activation('relu')(x)"
      ],
      "metadata": {
        "id": "lJFWZojZotZT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def dwsepcnn_block(ch):\n",
        "#   return keras.Sequential([\n",
        "#     keras.layers.DepthwiseConv2D((3,3), padding='same'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Activation('relu'),\n",
        "#     keras.layers.Conv2D(ch, (1,1), padding='same'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Activation('relu')\n",
        "#   ])"
      ],
      "metadata": {
        "id": "ibBCnYgvFjXz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution base"
      ],
      "metadata": {
        "id": "A5WYKf3UsVXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = keras.layers.Input((32,32,3))\n",
        "x = keras.layers.Conv2D(16, (3, 3), padding='same')(input)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = separable_conv(x, 16)\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "x = separable_conv(x, 48)\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "x = separable_conv(x, 96)\n",
        "x = separable_conv(x, 192)\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)"
      ],
      "metadata": {
        "id": "svHqZXotqufb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.Sequential([\n",
        "#     keras.layers.Input((32,32,3)),\n",
        "#     keras.layers.Conv2D(16, (3, 3), padding='same'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Activation('relu'),\n",
        "#     dwsepcnn_block(16),\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     dwsepcnn_block(48),\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     dwsepcnn_block(96),\n",
        "#     dwsepcnn_block(192),\n",
        "#     keras.layers.MaxPooling2D((2,2))])"
      ],
      "metadata": {
        "id": "_uNBUmZgFafw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification head"
      ],
      "metadata": {
        "id": "keR_OQ28sRhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Dense(10)(x)"
      ],
      "metadata": {
        "id": "ZGHJo4d5sUKe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.add(keras.layers.Flatten())\n",
        "# model.add(keras.layers.Dropout(0.2))\n",
        "# model.add(keras.layers.Dense(10))"
      ],
      "metadata": {
        "id": "LIwtUVUxIINA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.models import Model\n",
        "model = keras.models.Model(input, x)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "i1oevOBqsjSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873257a3-a4b2-4d28-c6e3-b3a25d86d7fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 16)        448       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 16)        64        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " depthwise_conv2d (Depthwis  (None, 32, 32, 16)        160       \n",
            " eConv2D)                                                        \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 16)        272       \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32, 32, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " depthwise_conv2d_1 (Depthw  (None, 16, 16, 16)        160       \n",
            " iseConv2D)                                                      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 16)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 48)        816       \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 16, 16, 48)        192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16, 16, 48)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 48)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " depthwise_conv2d_2 (Depthw  (None, 8, 8, 48)          480       \n",
            " iseConv2D)                                                      \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 48)          192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 48)          0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 96)          4704      \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 8, 8, 96)          384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8, 8, 96)          0         \n",
            "                                                                 \n",
            " depthwise_conv2d_3 (Depthw  (None, 8, 8, 96)          960       \n",
            " iseConv2D)                                                      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 8, 8, 96)          384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8, 8, 96)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 192)         18624     \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 8, 8, 192)         768       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 192)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                30730     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59530 (232.54 KB)\n",
            "Trainable params: 58442 (228.29 KB)\n",
            "Non-trainable params: 1088 (4.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_f = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss = loss_f, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_imgs, train_lbls, epochs=10, batch_size=32,\n",
        "                    validation_data=(val_imgs, val_lbls))\n",
        "\n",
        "model.export('cifar10')"
      ],
      "metadata": {
        "id": "qAaQPoDAtpJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e548598-b50a-4d72-e004-71c509cdfa1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 151s 94ms/step - loss: 1.6619 - accuracy: 0.4312 - val_loss: 1.3986 - val_accuracy: 0.5034\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 136s 87ms/step - loss: 1.2050 - accuracy: 0.5717 - val_loss: 1.2950 - val_accuracy: 0.5511\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 136s 87ms/step - loss: 1.0301 - accuracy: 0.6379 - val_loss: 1.1404 - val_accuracy: 0.6024\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 135s 86ms/step - loss: 0.9037 - accuracy: 0.6845 - val_loss: 0.9833 - val_accuracy: 0.6641\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 136s 87ms/step - loss: 0.8134 - accuracy: 0.7156 - val_loss: 0.9879 - val_accuracy: 0.6612\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 137s 87ms/step - loss: 0.7536 - accuracy: 0.7362 - val_loss: 0.8159 - val_accuracy: 0.7172\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 141s 90ms/step - loss: 0.7015 - accuracy: 0.7536 - val_loss: 0.8060 - val_accuracy: 0.7211\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 142s 91ms/step - loss: 0.6559 - accuracy: 0.7707 - val_loss: 0.8276 - val_accuracy: 0.7168\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 137s 88ms/step - loss: 0.6191 - accuracy: 0.7836 - val_loss: 0.7857 - val_accuracy: 0.7376\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 141s 90ms/step - loss: 0.5882 - accuracy: 0.7943 - val_loss: 0.8108 - val_accuracy: 0.7333\n",
            "Saved artifact at 'cifar10'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_1')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135954260309968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260310544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954278586064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260309776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260307280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260310352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260310736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260311696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260311312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260309392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260309200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260310160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260311120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954260311504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259870160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259871312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259871504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259871696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259873808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259874384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259874000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259873424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259873232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259872464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259874960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259875536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259873616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259874192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259875728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259875920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259876880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259877456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259877072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259876496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259874576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259876304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259878416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259878992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259878800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259878032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259876688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259877840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259879568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259880144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259879760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259878224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259878608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259879184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259880720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259881296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259880912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259880336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259879952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259881488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135956627434832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135954259882640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize the model\n",
        "We need a sample from the training dataset to run inference on for full integer\n",
        "quantization, because the zero point and scale need to be calculated for the activations."
      ],
      "metadata": {
        "id": "fSBbV6M3zswR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_ds = tf.data.Dataset.from_tensor_slices(train_imgs)\n",
        "def representative_data_gen():\n",
        "  for i_value in cifar_ds.batch(1).take(1000):\n",
        "    i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
        "    yield [i_value_f32]\n",
        "\n",
        "tfl_conv = tf.lite.TFLiteConverter.from_saved_model('cifar10')\n",
        "tfl_conv.representative_dataset = \\\n",
        "  tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "tfl_conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tfl_conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "tfl_conv.inference_input_type = tf.int8\n",
        "tfl_conv.inference_output_type = tf.int8"
      ],
      "metadata": {
        "id": "ZeJUi9SkzsKR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfl_model = tfl_conv.convert()\n",
        "print(len(tfl_model))"
      ],
      "metadata": {
        "id": "_wuHmDxc-_DO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55af0c8-f326-4a1a-99d9-41e049f7aefb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the quantized model using the validation dataset"
      ],
      "metadata": {
        "id": "R9qGwxdQ_O3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfl_interp = tf.lite.Interpreter(model_content=tfl_model)\n",
        "tfl_interp.allocate_tensors()\n",
        "\n",
        "i_details = tfl_interp.get_input_details()[0]\n",
        "o_details = tfl_interp.get_output_details()[0]\n",
        "\n",
        "# print(len(tfl_interp.get_input_details()))\n",
        "# print(len(tfl_interp.get_output_details()))\n",
        "\n",
        "i_quant = i_details['quantization_parameters']\n",
        "o_quant = o_details['quantization_parameters']\n",
        "i_scale = i_quant['scales'][0]\n",
        "i_zero_point = i_quant['zero_points'][0]\n",
        "o_scale = o_quant['scales'][0]\n",
        "o_zero_point = o_quant['zero_points'][0]"
      ],
      "metadata": {
        "id": "hzRM7-R6_NoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13937309-128e-4e35-c952-db0901b4a4d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(i_data):\n",
        "  input_data = i_data.reshape((1, 32, 32, 3))\n",
        "  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n",
        "  i_value_f32 = i_value_f32 / i_scale + i_zero_point\n",
        "  i_value_s8 = tf.cast(i_value_f32, dtype=tf.int8)\n",
        "\n",
        "  tfl_interp.set_tensor(i_details['index'], i_value_s8)\n",
        "  tfl_interp.invoke()\n",
        "  o_pred = tfl_interp.get_tensor(o_details['index'])[0]\n",
        "\n",
        "  return (o_pred - o_zero_point) * o_scale"
      ],
      "metadata": {
        "id": "HBqpG7_MO9IG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct_samples = 0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for i_value, o_value in zip(val_imgs, val_lbls):\n",
        "  o_pred_f32 = classify(i_value)\n",
        "  if np.argmax(o_pred_f32) == o_value:\n",
        "    num_correct_samples += 1\n",
        "\n",
        "total_samples = len(list(val_imgs))\n",
        "print('Accuracy: ', num_correct_samples/total_samples)"
      ],
      "metadata": {
        "id": "QRk-5AKhQs-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e56cbf8-4238-4ccd-d46c-6a7690046743"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cifar10.tflite', 'wb') as file:\n",
        "  file.write(tfl_model)\n",
        "\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i cifar10.tflite > model.h\n",
        "!sed -i 's/unsigned char/const unsigned char/g' model.h\n",
        "!sed -i 's/const/alignas(8) const/g' model.h"
      ],
      "metadata": {
        "id": "DhjSwXp5R-df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64324ce-9562-401a-9632-d43dd665da57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,267 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,627 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,797 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [88.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Fetched 30.0 MB in 3s (8,592 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization aware training\n",
        "https://www.tensorflow.org/model_optimization/guide/quantization/training_example"
      ],
      "metadata": {
        "id": "-mgGMUA8tPDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# quantize_model requires a recompile\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "                      loss=loss_f,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "id": "ajaXBsIgtTOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d791a3-752a-45f0-e31e-59f7eb3638cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " quantize_layer (QuantizeLa  (None, 32, 32, 3)         3         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrap  (None, 32, 32, 16)        481       \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_batch_normalization   (None, 32, 32, 16)        65        \n",
            " (QuantizeWrapperV2)                                             \n",
            "                                                                 \n",
            " quant_activation (Quantize  (None, 32, 32, 16)        3         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_depthwise_conv2d (Qu  (None, 32, 32, 16)        163       \n",
            " antizeWrapperV2)                                                \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 32, 32, 16)        65        \n",
            " 1 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_1 (Quanti  (None, 32, 32, 16)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWr  (None, 32, 32, 16)        305       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 32, 32, 16)        65        \n",
            " 2 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_2 (Quanti  (None, 32, 32, 16)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quant  (None, 16, 16, 16)        1         \n",
            " izeWrapperV2)                                                   \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_1 (  (None, 16, 16, 16)        163       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 16, 16, 16)        65        \n",
            " 3 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_3 (Quanti  (None, 16, 16, 16)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWr  (None, 16, 16, 48)        913       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 16, 16, 48)        193       \n",
            " 4 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_4 (Quanti  (None, 16, 16, 48)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Qua  (None, 8, 8, 48)          1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_2 (  (None, 8, 8, 48)          483       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 48)          193       \n",
            " 5 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_5 (Quanti  (None, 8, 8, 48)          3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWr  (None, 8, 8, 96)          4897      \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 96)          385       \n",
            " 6 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_6 (Quanti  (None, 8, 8, 96)          3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_3 (  (None, 8, 8, 96)          963       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 96)          385       \n",
            " 7 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_7 (Quanti  (None, 8, 8, 96)          3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_4 (QuantizeWr  (None, 8, 8, 192)         19009     \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 192)         769       \n",
            " 8 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_8 (Quanti  (None, 8, 8, 192)         3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Qua  (None, 4, 4, 192)         1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWra  (None, 3072)              1         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWra  (None, 3072)              1         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrapp  (None, 10)                30735     \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60332 (235.67 KB)\n",
            "Trainable params: 58442 (228.29 KB)\n",
            "Non-trainable params: 1890 (7.38 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All layers are now prefixed by \"quant\". Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8)."
      ],
      "metadata": {
        "id": "m4hHrPuzvfEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_subset = train_imgs[:1000]\n",
        "train_labels_subset = train_lbls[:1000]\n",
        "\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=64, epochs=2, validation_data=(val_imgs, val_lbls))\n",
        "\n",
        "_, baseline_model_accuracy = model.evaluate(test_imgs, test_lbls, verbose=0)\n",
        "#_, tfl_model_accuracy = tfl_model.evaluate(test_imgs, test_lbls, verbose=0)\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(test_imgs, test_lbls, verbose=0)\n"
      ],
      "metadata": {
        "id": "ZjjJ89rIviIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa4808c-06b8-430b-b2b3-bde0e586799f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "16/16 [==============================] - 14s 642ms/step - loss: 0.4970 - accuracy: 0.8220 - val_loss: 0.7851 - val_accuracy: 0.7329\n",
            "Epoch 2/2\n",
            "16/16 [==============================] - 9s 566ms/step - loss: 0.3198 - accuracy: 0.9110 - val_loss: 0.7662 - val_accuracy: 0.7465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Baseline test accuracy: ', baseline_model_accuracy)\n",
        "#print('Quant test accuracy: ', tfl_model_accuracy)\n",
        "print('Quant aware test accuracy: ', q_aware_model_accuracy)"
      ],
      "metadata": {
        "id": "3alci3dhe-Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858c312f-fd3e-4ccd-f85f-3fa855082950"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy:  0.7174999713897705\n",
            "Quant aware test accuracy:  0.7404999732971191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantize q-aware model"
      ],
      "metadata": {
        "id": "dP-K2_kZpucz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfl_conv = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "tfl_conv.representative_dataset = \\\n",
        "  tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "tfl_conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tfl_conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "tfl_conv.inference_input_type = tf.int8\n",
        "tfl_conv.inference_output_type = tf.int8\n",
        "\n",
        "quantized_q_aware_model = tfl_conv.convert()"
      ],
      "metadata": {
        "id": "4e7d-SNXn_pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc1d0c8-ce2f-40a6-9172-711605dc86b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cifar10_q_aware.tflite', 'wb') as file:\n",
        "  file.write(quantized_q_aware_model)\n",
        "\n",
        "!xxd -i 'cifar10_q_aware.tflite' > q_aware_model.h\n",
        "!sed -i 's/unsigned char/const unsigned char/g' q_aware_model.h\n",
        "!sed -i 's/const/alignas(8) const/g' q_aware_model.h"
      ],
      "metadata": {
        "id": "hhnST2kpo3qV"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}