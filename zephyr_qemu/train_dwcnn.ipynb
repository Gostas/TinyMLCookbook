{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization numpy tensorflow tf-keras"
      ],
      "metadata": {
        "id": "7JbfX1Zvawvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d1f9a2-4df6-48e6-b05f-6c9976e8478e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Collecting tf-keras\n",
            "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-model-optimization) (0.1.9)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.12/dist-packages (from tensorflow-model-optimization) (1.17.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-keras\n",
            "Successfully installed tf-keras-2.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train base model on CIFAR-10"
      ],
      "metadata": {
        "id": "Y1DgRUSelsgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RQF-lRsKn68H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(train_imgs, train_lbls), (val_imgs, val_lbls) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_imgs, val_imgs = train_imgs/255.0, val_imgs/255.0\n",
        "\n",
        "test_imgs, test_lbls = val_imgs[-2000:], val_lbls[-2000:]\n",
        "val_imgs, val_lbls = val_imgs[:8000], val_lbls[:8000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tf_keras as keras\n",
        "\n",
        "def separable_conv(i, ch):\n",
        "  x = keras.layers.DepthwiseConv2D((3,3), padding='same')(i)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  x = keras.layers.Activation('relu')(x)\n",
        "  x = keras.layers.Conv2D(ch, (1,1), padding='same')(x)\n",
        "  x = keras.layers.BatchNormalization()(x)\n",
        "  return keras.layers.Activation('relu')(x)"
      ],
      "metadata": {
        "id": "lJFWZojZotZT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def dwsepcnn_block(ch):\n",
        "#   return keras.Sequential([\n",
        "#     keras.layers.DepthwiseConv2D((3,3), padding='same'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Activation('relu'),\n",
        "#     keras.layers.Conv2D(ch, (1,1), padding='same'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Activation('relu')\n",
        "#   ])"
      ],
      "metadata": {
        "id": "ibBCnYgvFjXz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution base"
      ],
      "metadata": {
        "id": "A5WYKf3UsVXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = keras.layers.Input((32,32,3))\n",
        "x = keras.layers.Conv2D(16, (3, 3), padding='same')(input)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "x = separable_conv(x, 16)\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "x = separable_conv(x, 48)\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)\n",
        "x = separable_conv(x, 96)\n",
        "x = separable_conv(x, 192)\n",
        "x = keras.layers.MaxPooling2D((2,2))(x)"
      ],
      "metadata": {
        "id": "svHqZXotqufb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.Sequential([\n",
        "#     keras.layers.Input((32,32,3)),\n",
        "#     keras.layers.Conv2D(16, (3, 3), padding='same'),\n",
        "#     keras.layers.BatchNormalization(),\n",
        "#     keras.layers.Activation('relu'),\n",
        "#     dwsepcnn_block(16),\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     dwsepcnn_block(48),\n",
        "#     keras.layers.MaxPooling2D((2,2)),\n",
        "#     dwsepcnn_block(96),\n",
        "#     dwsepcnn_block(192),\n",
        "#     keras.layers.MaxPooling2D((2,2))])"
      ],
      "metadata": {
        "id": "_uNBUmZgFafw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification head"
      ],
      "metadata": {
        "id": "keR_OQ28sRhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "x = keras.layers.Dense(10)(x)"
      ],
      "metadata": {
        "id": "ZGHJo4d5sUKe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.add(keras.layers.Flatten())\n",
        "# model.add(keras.layers.Dropout(0.2))\n",
        "# model.add(keras.layers.Dense(10))"
      ],
      "metadata": {
        "id": "LIwtUVUxIINA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.models import Model\n",
        "model = keras.models.Model(input, x)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "i1oevOBqsjSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50112803-3229-4860-faf0-f7a9e8711cca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 16)        448       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 16)        64        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " depthwise_conv2d (Depthwis  (None, 32, 32, 16)        160       \n",
            " eConv2D)                                                        \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 16)        272       \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32, 32, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " depthwise_conv2d_1 (Depthw  (None, 16, 16, 16)        160       \n",
            " iseConv2D)                                                      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 16)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 48)        816       \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 16, 16, 48)        192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16, 16, 48)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 48)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " depthwise_conv2d_2 (Depthw  (None, 8, 8, 48)          480       \n",
            " iseConv2D)                                                      \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 48)          192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 48)          0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 96)          4704      \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 8, 8, 96)          384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8, 8, 96)          0         \n",
            "                                                                 \n",
            " depthwise_conv2d_3 (Depthw  (None, 8, 8, 96)          960       \n",
            " iseConv2D)                                                      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 8, 8, 96)          384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8, 8, 96)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 192)         18624     \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 8, 8, 192)         768       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 192)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                30730     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59530 (232.54 KB)\n",
            "Trainable params: 58442 (228.29 KB)\n",
            "Non-trainable params: 1088 (4.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_f = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss = loss_f, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_imgs, train_lbls, epochs=10, batch_size=32,\n",
        "                    validation_data=(val_imgs, val_lbls))\n",
        "\n",
        "model.export('cifar10')"
      ],
      "metadata": {
        "id": "qAaQPoDAtpJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ef49a0-6227-485d-fe48-a14ae807dbf0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 54s 33ms/step - loss: 1.5895 - accuracy: 0.4614 - val_loss: 1.2446 - val_accuracy: 0.5620\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 1.1111 - accuracy: 0.6102 - val_loss: 1.0663 - val_accuracy: 0.6246\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 0.9515 - accuracy: 0.6681 - val_loss: 1.0373 - val_accuracy: 0.6439\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: 0.8460 - accuracy: 0.7035 - val_loss: 0.9985 - val_accuracy: 0.6680\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 0.7780 - accuracy: 0.7299 - val_loss: 0.8653 - val_accuracy: 0.7048\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: 0.7189 - accuracy: 0.7497 - val_loss: 0.8111 - val_accuracy: 0.7172\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 53s 34ms/step - loss: 0.6698 - accuracy: 0.7643 - val_loss: 0.8153 - val_accuracy: 0.7237\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 53s 34ms/step - loss: 0.6365 - accuracy: 0.7782 - val_loss: 0.7809 - val_accuracy: 0.7312\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5976 - accuracy: 0.7920 - val_loss: 0.7565 - val_accuracy: 0.7427\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 52s 34ms/step - loss: 0.5696 - accuracy: 0.8002 - val_loss: 0.7711 - val_accuracy: 0.7385\n",
            "Saved artifact at 'cifar10'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_1')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135022766177360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766177936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766176784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766176976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766176016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766177552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766177168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766178128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766176592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766031312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766031696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766031504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135024024462608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766177744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766031888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766033232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766033424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766033616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766035728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766036304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766035920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766035344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766034576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766035152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766036880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766037456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766035536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766036496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766037648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766037840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766038800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766039376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766038992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766038416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766036112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766038224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766040336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766040912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766040720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766039952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766038608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766039760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766041488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766042064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766041680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766040144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766041104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766040528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766042640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766043216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766042832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766041872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766042256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766043408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766044560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135022766045136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize the model\n",
        "We need a sample from the training dataset to run inference on for full integer\n",
        "quantization, because the zero point and scale need to be calculated for the activations."
      ],
      "metadata": {
        "id": "fSBbV6M3zswR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_ds = tf.data.Dataset.from_tensor_slices(train_imgs)\n",
        "def representative_data_gen():\n",
        "  for i_value in cifar_ds.batch(1).take(1000):\n",
        "    i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
        "    yield [i_value_f32]\n",
        "\n",
        "tfl_conv = tf.lite.TFLiteConverter.from_saved_model('cifar10')\n",
        "tfl_conv.representative_dataset = \\\n",
        "  tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "tfl_conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tfl_conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "tfl_conv.inference_input_type = tf.int8\n",
        "tfl_conv.inference_output_type = tf.int8"
      ],
      "metadata": {
        "id": "ZeJUi9SkzsKR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfl_model = tfl_conv.convert()\n",
        "print(len(tfl_model))"
      ],
      "metadata": {
        "id": "_wuHmDxc-_DO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f31bfea-d7e2-45d5-cc0e-90a59140c68d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the quantized model using the validation dataset"
      ],
      "metadata": {
        "id": "R9qGwxdQ_O3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfl_interp = tf.lite.Interpreter(model_content=tfl_model)\n",
        "tfl_interp.allocate_tensors()\n",
        "\n",
        "i_details = tfl_interp.get_input_details()[0]\n",
        "o_details = tfl_interp.get_output_details()[0]\n",
        "\n",
        "# print(len(tfl_interp.get_input_details()))\n",
        "# print(len(tfl_interp.get_output_details()))\n",
        "\n",
        "i_quant = i_details['quantization_parameters']\n",
        "o_quant = o_details['quantization_parameters']\n",
        "i_scale = i_quant['scales'][0]\n",
        "i_zero_point = i_quant['zero_points'][0]\n",
        "o_scale = o_quant['scales'][0]\n",
        "o_zero_point = o_quant['zero_points'][0]"
      ],
      "metadata": {
        "id": "hzRM7-R6_NoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0bca01-2b99-4ab1-af21-601789aaf2f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(i_data):\n",
        "  input_data = i_data.reshape((1, 32, 32, 3))\n",
        "  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n",
        "  i_value_f32 = i_value_f32 / i_scale + i_zero_point\n",
        "  i_value_s8 = tf.cast(i_value_f32, dtype=tf.int8)\n",
        "\n",
        "  tfl_interp.set_tensor(i_details['index'], i_value_s8)\n",
        "  tfl_interp.invoke()\n",
        "  o_pred = tfl_interp.get_tensor(o_details['index'])[0]\n",
        "\n",
        "  return (o_pred - o_zero_point) * o_scale"
      ],
      "metadata": {
        "id": "HBqpG7_MO9IG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct_samples = 0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for i_value, o_value in zip(val_imgs, val_lbls):\n",
        "  o_pred_f32 = classify(i_value)\n",
        "  if np.argmax(o_pred_f32) == o_value:\n",
        "    num_correct_samples += 1\n",
        "\n",
        "total_samples = len(list(val_imgs))\n",
        "print('Accuracy: ', num_correct_samples/total_samples)"
      ],
      "metadata": {
        "id": "QRk-5AKhQs-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dfbd9f9-62a7-48eb-b221-1534c5199a50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.7375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cifar10.tflite', 'wb') as file:\n",
        "  file.write(tfl_model)\n",
        "\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i cifar10.tflite > model.h\n",
        "!sed -i 's/unsigned char/const unsigned char/g' model.h\n",
        "!sed -i 's/const/alignas(8) const/g' model.h"
      ],
      "metadata": {
        "id": "DhjSwXp5R-df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2766815-7e9d-4c73-ebea-89a61b4397c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.2 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,823 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,479 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,414 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,148 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,847 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Fetched 29.1 MB in 2s (12.4 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization aware training\n",
        "https://www.tensorflow.org/model_optimization/guide/quantization/training_example"
      ],
      "metadata": {
        "id": "-mgGMUA8tPDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# quantize_model requires a recompile\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "                      loss=loss_f,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "id": "ajaXBsIgtTOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de239a24-3c2b-4622-9fd6-43e39328b79c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " quantize_layer (QuantizeLa  (None, 32, 32, 3)         3         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrap  (None, 32, 32, 16)        481       \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_batch_normalization   (None, 32, 32, 16)        65        \n",
            " (QuantizeWrapperV2)                                             \n",
            "                                                                 \n",
            " quant_activation (Quantize  (None, 32, 32, 16)        3         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_depthwise_conv2d (Qu  (None, 32, 32, 16)        163       \n",
            " antizeWrapperV2)                                                \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 32, 32, 16)        65        \n",
            " 1 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_1 (Quanti  (None, 32, 32, 16)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWr  (None, 32, 32, 16)        305       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 32, 32, 16)        65        \n",
            " 2 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_2 (Quanti  (None, 32, 32, 16)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quant  (None, 16, 16, 16)        1         \n",
            " izeWrapperV2)                                                   \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_1 (  (None, 16, 16, 16)        163       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 16, 16, 16)        65        \n",
            " 3 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_3 (Quanti  (None, 16, 16, 16)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWr  (None, 16, 16, 48)        913       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 16, 16, 48)        193       \n",
            " 4 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_4 (Quanti  (None, 16, 16, 48)        3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Qua  (None, 8, 8, 48)          1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_2 (  (None, 8, 8, 48)          483       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 48)          193       \n",
            " 5 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_5 (Quanti  (None, 8, 8, 48)          3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWr  (None, 8, 8, 96)          4897      \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 96)          385       \n",
            " 6 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_6 (Quanti  (None, 8, 8, 96)          3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_depthwise_conv2d_3 (  (None, 8, 8, 96)          963       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 96)          385       \n",
            " 7 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_7 (Quanti  (None, 8, 8, 96)          3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_4 (QuantizeWr  (None, 8, 8, 192)         19009     \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_batch_normalization_  (None, 8, 8, 192)         769       \n",
            " 8 (QuantizeWrapperV2)                                           \n",
            "                                                                 \n",
            " quant_activation_8 (Quanti  (None, 8, 8, 192)         3         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Qua  (None, 4, 4, 192)         1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWra  (None, 3072)              1         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWra  (None, 3072)              1         \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrapp  (None, 10)                30735     \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60332 (235.67 KB)\n",
            "Trainable params: 58442 (228.29 KB)\n",
            "Non-trainable params: 1890 (7.38 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All layers are now prefixed by \"quant\". Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8)."
      ],
      "metadata": {
        "id": "m4hHrPuzvfEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_subset = train_imgs[:1000]\n",
        "train_labels_subset = train_lbls[:1000]\n",
        "\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=64, epochs=2, validation_data=(val_imgs, val_lbls))\n",
        "\n",
        "_, baseline_model_accuracy = model.evaluate(test_imgs, test_lbls, verbose=0)\n",
        "#_, tfl_model_accuracy = tfl_model.evaluate(test_imgs, test_lbls, verbose=0)\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(test_imgs, test_lbls, verbose=0)\n"
      ],
      "metadata": {
        "id": "ZjjJ89rIviIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46719d76-0779-47d9-d7d1-41821d709018"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "16/16 [==============================] - 7s 275ms/step - loss: 0.4983 - accuracy: 0.8290 - val_loss: 0.8287 - val_accuracy: 0.7254\n",
            "Epoch 2/2\n",
            "16/16 [==============================] - 4s 254ms/step - loss: 0.3026 - accuracy: 0.9070 - val_loss: 0.7987 - val_accuracy: 0.7324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Baseline test accuracy: ', baseline_model_accuracy)\n",
        "#print('Quant test accuracy: ', tfl_model_accuracy)\n",
        "print('Quant aware test accuracy: ', q_aware_model_accuracy)"
      ],
      "metadata": {
        "id": "3alci3dhe-Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a95d2c-0a47-486b-c7e1-115818522a34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy:  0.7294999957084656\n",
            "Quant aware test accuracy:  0.7170000076293945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantize q-aware model"
      ],
      "metadata": {
        "id": "dP-K2_kZpucz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfl_conv = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "tfl_conv.representative_dataset = \\\n",
        "  tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "tfl_conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tfl_conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "tfl_conv.inference_input_type = tf.int8\n",
        "tfl_conv.inference_output_type = tf.int8\n",
        "\n",
        "quantized_q_aware_model = tfl_conv.convert()"
      ],
      "metadata": {
        "id": "4e7d-SNXn_pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd57ce4-ac3a-46a4-93aa-fc965bb70a61"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:863: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cifar10_q_aware.tflite', 'wb') as file:\n",
        "  file.write(quantized_q_aware_model)\n",
        "\n",
        "!xxd -i 'cifar10_q_aware.tflite' > q_aware_model.h\n",
        "!sed -i 's/unsigned char/const unsigned char/g' q_aware_model.h\n",
        "!sed -i 's/const/alignas(8) const/g' q_aware_model.h"
      ],
      "metadata": {
        "id": "hhnST2kpo3qV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def array_to_str(data):\n",
        "    NUM_COLS = 12\n",
        "    val_string = ''\n",
        "    for i, val in enumerate(data):\n",
        "        val_string += str(val)\n",
        "        if (i+1) < len(data):\n",
        "            val_string += ','\n",
        "        if (i+1) % NUM_COLS == 0:\n",
        "            val_string += '\\n'\n",
        "    return val_string\n",
        "\n",
        "def gen_h_file(size, data, ilabel):\n",
        "    str_out = f'int8_t g_test[] = '\n",
        "    str_out += \"\\n{\\n\"\n",
        "    str_out += f'{data}'\n",
        "    str_out += '};\\n'\n",
        "    str_out += f'const int g_test_len = {size};\\n'\n",
        "    str_out += f'const int g_test_ilabel = {ilabel};\\n'\n",
        "    return str_out\n",
        "\n",
        "imgs = list(zip(val_imgs, val_lbls))\n",
        "cols = ['Image', 'Label']\n",
        "df = pd.DataFrame(imgs, columns=cols)\n",
        "\n",
        "ship_samples = df[df['Label'] == 8]\n",
        "\n",
        "c_code = ''\n",
        "\n",
        "for index, row in ship_samples.iterrows():\n",
        "    i_value = np.asarray(row['Image'].tolist())\n",
        "    o_value = np.asarray(row['Label'].tolist())\n",
        "    o_pred_f32 = classify(i_value)\n",
        "\n",
        "    if np.argmax(o_pred_f32) == o_value:\n",
        "        i_value_f32 = i_value / i_scale + i_zero_point\n",
        "        i_value_s8 = i_value_f32.astype(dtype=np.int8)\n",
        "        i_value_s8 = i_value_s8.ravel()\n",
        "        # Generate a string from the numpy array\n",
        "        val_string  = array_to_str(i_value_s8)\n",
        "        # Generate the C header file\n",
        "        c_code  = gen_h_file(i_value_s8.size, val_string, \"8\")\n",
        "        break\n",
        "\n",
        "with open('input.h', 'w') as file:\n",
        "    file.write(c_code)"
      ],
      "metadata": {
        "id": "55G3d3A4T-Ew"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}