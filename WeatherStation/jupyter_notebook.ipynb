{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwo_hist import retrieve_hist_data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import pandas as pd\n",
    "from numpy import mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '934cf43240344cfb8ea21455232111'\n",
    "location_list = ['M5B0C3']\n",
    "#hist_df = retrieve_hist_data(api_key, location_list, '19-NOV-2017', '19-NOV-2023', frequency=1, location_label=False, export_csv=True, store_df=False)\n",
    "hist_df = pd.read_csv('M5B0C3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = hist_df.tempC.astype(float).to_list()\n",
    "h_list = hist_df.humidity.astype(float).to_list()\n",
    "s_list = hist_df.totalSnow_cm.astype(float).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(snow, thershold):\n",
    "    if snow > thershold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "s_bin_list = [binarize(snow, 0.5) for snow in s_list]\n",
    "\n",
    "cm = colormaps['gray_r']\n",
    "sc = plt.scatter(t_list, h_list, c=s_bin_list, cmap=cm, label='Snow')\n",
    "plt.colorbar(sc)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Snowfall')\n",
    "plt.xlabel('Temperature (*C)')\n",
    "plt.ylabel('Humidity (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_label(snow):\n",
    "    if snow > 0.5:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "    \n",
    "labels_list = [gen_label(snow) for snow in s_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_header = ['Temp0', 'Temp1', 'Temp2', 'Humi0', 'Humi1', 'Humi2', 'Snow']\n",
    "\n",
    "dataset_df = pd.DataFrame(list(zip(t_list[:-2], t_list[1:-1], t_list[2:], h_list[:-2], h_list[1:-1], h_list[2:], labels_list[2:])), columns=csv_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = dataset_df[dataset_df['Snow'] == 'No']\n",
    "df1 = dataset_df[dataset_df['Snow'] == 'Yes']\n",
    "\n",
    "if len(df1.index) < len(df0.index):\n",
    "    df0_sub = df0.sample(len(df1.index))\n",
    "    dataset_df = pd.concat([df0_sub, df1])\n",
    "else:\n",
    "    df1_sub = df1.sample(len(df0.index))\n",
    "    dataset_df = pd.concat([df1_sub, df0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = dataset_df['Temp0'].tolist() + dataset_df['Temp2'].tail(2).tolist()\n",
    "h_list = dataset_df['Humi0'].tolist() + dataset_df['Humi2'].tail(2).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale input features with Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_avg = mean(t_list)\n",
    "h_avg = mean(h_list)\n",
    "t_std = std(t_list)\n",
    "h_std = std(h_list)\n",
    "\n",
    "print('COPY ME!')\n",
    "print('Temperature - [MEAN, STD] ', round(t_avg, 5), round(t_std, 5))\n",
    "print('Humidity - [MEAN, STD] ', round(h_avg, 5), round(h_std, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(val, avg, std):\n",
    "    return (val - avg)/std\n",
    "\n",
    "dataset_df['Temp0'] = dataset_df['Temp0'].apply(lambda x: scaling(x, t_avg, t_std))\n",
    "dataset_df['Temp1'] = dataset_df['Temp1'].apply(lambda x: scaling(x, t_avg, t_std))\n",
    "dataset_df['Temp2'] = dataset_df['Temp2'].apply(lambda x: scaling(x, t_avg, t_std))\n",
    "\n",
    "dataset_df['Humi0'] = dataset_df['Humi0'].apply(lambda x: scaling(x, h_avg, h_std))\n",
    "dataset_df['Humi1'] = dataset_df['Humi1'].apply(lambda x: scaling(x, h_avg, h_std))\n",
    "dataset_df['Humi2'] = dataset_df['Humi2'].apply(lambda x: scaling(x, h_avg, h_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize scaled inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = dataset_df.columns.values[:6]\n",
    "l_name = dataset_df.columns.values[6:7]\n",
    "x = dataset_df[f_names]\n",
    "y = dataset_df[l_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(y.Snow)\n",
    "y_encoded = labelencoder.transform(y.Snow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 80% train, 10% validate, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate_test, y_train, y_validate_test = train_test_split(x, y_encoded, test_size=0.2, random_state=1)\n",
    "x_test, x_validate, y_test, y_validate = train_test_split(x_validate_test, y_validate_test, test_size=0.5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model (binary classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network\n",
    "Input (6 features) -> [Fully connected layer (12 neurons) + Relu] -> [Dropout 20%] -> [Fully connected (1 neuron) + sigmoid] -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(12, activation='relu', input_shape=(len(f_names),)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=75, batch_size=64, \n",
    "                    validation_data=(x_validate, y_validate), \n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "acc_train = history.history['accuracy']\n",
    "acc_val = history.history['val_accuracy']\n",
    "epochs = range(1, len(history.history['loss'])+1)\n",
    "\n",
    "def plot_train_val_history(x, train, val, type_txt):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(x, train, 'g', label='Training ' + type_txt)\n",
    "    plt.plot(x, val, 'b', label='Validation ' + type_txt)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(type_txt)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_val_history(epochs, loss_train, loss_val, 'Loss')\n",
    "plot_train_val_history(epochs, acc_train, acc_val, 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('snow_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred = (y_test_pred > 0.5).astype('int32') # binarize output probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "q_aware_model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_subset, _, y_train_subset, _ = train_test_split(x_train, y_train, test_size=0.25, random_state=8)\n",
    "\n",
    "q_aware_model.fit(x_train_subset, y_train_subset, batch_size=64, epochs=1, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get operations used in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def func(x):\n",
    "    return tflite_model_quant(x)\n",
    "\n",
    "model_func = func.get_concrete_function(pd.DataFrame(pd.DataFrame([(0,0,0,0,0,0)], columns=csv_header[:6])))\n",
    "ops = model_func.graph.get_operations()\n",
    "\n",
    "unique_ops = set()\n",
    "\n",
    "for op in ops:\n",
    "    unique_ops.add(op)\n",
    "\n",
    "for op in unique_ops:\n",
    "    print(f'Name: {op.name}')\n",
    "    print(f'Op: {op.type}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "index_names = ['Actual No Snow', 'Actual Snow']\n",
    "column_names = ['Predicted No Snow', 'Predicted Snow']\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index=index_names, columns=column_names)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN,TP,FN,FP = cm[0][0],cm[1][1],cm[1][0],cm[0][1]\n",
    "accur = (TP+TN)/(TP+TN+FN+FP)\n",
    "precis = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "f_score = 2*precis*recall/(precis+recall)\n",
    "_, q_aware_model_acc = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Accuracy: {round(accur, 3)}')\n",
    "print(f'Quant accuracy: {round(q_aware_model_acc, 3)}')\n",
    "print(f'Precision: {round(precis, 3)}')\n",
    "print(f'Recall: {round(recall, 3)}')\n",
    "print(f'Specificity: {round(specificity, 3)}')\n",
    "print(f'F-score: {round(f_score, 3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to TensorFlow Lite and byte array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    data = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "    for i_value in data.batch(1).take(100):\n",
    "        i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
    "        yield [i_value_f32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the TFLite converter\n",
    "#converter = tf.lite.TFLiteConverter.from_saved_model('snow_forecast')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# convert to TFLite format\n",
    "tflite_model_quant = converter.convert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View TfLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'snow_model_q_aware.tflite' at http://localhost:18518\n",
      "Serving 'snow_model.tflite' at http://localhost:23479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 23479)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/kpaps/.config/wslu/baseexec': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "\n",
    "netron.start('snow_model_q_aware.tflite')\n",
    "netron.start('snow_model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and convert it to C byte-array\n",
    "# open('snow_model.tflite', 'wb').write(tflite_model_quant)\n",
    "# !xxd -i 'snow_model.tflite' > 'model.h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open('snow_model_q_aware.tflite', 'wb').write(tflite_model_quant)\n",
    "# !xxd -i 'snow_model_q_aware.tflite' > 'q_aware_model.h'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
